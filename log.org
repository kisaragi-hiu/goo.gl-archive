#+title: Log

This is written after the fact, by me looking at the commit log and writing down some more details that I remember.

* 2024-07-19

Saw the shutdown news, thought I sort of know where to start so started trying

Iterate through goo.gl links with a generator function; I call them “slugs” for now

Slugs are a-zA-Z0-9, for apparently 1~6 digits. That seems like a very large space to brute force through, but I guess it's better than nothing. niconico still not being fully restored (and for people outside Japan, still inaccessible because DWANGO believes in region blocking as a security measure) made me feel more about losing information, I guess

Store it with SQLite; use Bun for the builting SQLite driver

Enable WAL so I can have multiple writers; set timeout so they wait a bit instead of returning busy

Implement =--init= as a resume point so the scraper can be stopped and restarted

Implement =--until= so that an invocation of the scraper could be more predictable. Init and until allow us to scrape in blocks.

Implement =--prefix=, as there are other namespaces under goo.gl like goo.gl/fb (that's the main one that I care about)

Add the external slugs mechanism. Write out the slugs without the value in a JSON file so it's possibly easier to synchronize between machines that are running the scraper.

Allow passing in an array of slugs to store. Use this to store every goo.gl link I could find on lkml. This was done by downloading the =mbox.gz= for the search from lore.kernel.org, running

#+begin_src sh
grep -o --text 'https\?://[^ ]*goo.gl[^ ]*' results-goo.gl.mbox > instances
#+end_src

which yields about 600~800 instances after removing duplicates. Then I manually cleaned up some oddities that are the result of the imperfect grep or line wrapping on “=” breaking some URLs up.

404 are stored as NULL entries, as that means the slug has no value.
Other status codes are stored for further processing. 400 is used for blocked links and other purposes.

Implement a mechanism for merging the SQLite databases.

Start running the scraping process on my server and my desktop. Multiple processes are launched to scrape multiple blocks at once.

* 2024-07-20

Use concurrently to run each process instead of just using the =command &= shell syntax. This allows individual jobs to fail then get automatically restarted.

Turns out some goo.gl links resolve to yet another goo.gl link. Some long URLs also refer to goo.gl as well. Implement a mechanism to extract such URLs that are mentioned in other stored URLs, and a command to scrape them.

When scraping slugs from an array, randomize the order so that multiple jobs are less likely to be scraping the same slugs. This could've been avoided if I just ran the scrape function multiple times asynchronously in the same process.

Try to reduce memory usage on A because A is very memory limited.

Give B a lot more tasks as I realize the bottleneck at this moment was memory use, not network speed. A is in a data center while B relies on a 4G cellular network.

* 2024-07-21

Port the scraper script to Node (running through privatenumber/tsx) and better-sqlite3, as I thought it might provide lower memory usage. I also did this to make it possible to run the scraper on my Android phone, as Bun does not (yet) have an Android / Termux build.

The Node version works, but it actually uses a bit more memory, and for A that really matters. A has only a gigabyte of memory. When trying to run it on my phone, it also keeps getting killed, presumably because it's using too much memory.

I struggle a bit to get the same file to work for both Node and Bun while also keeping TypeScript happy. Something about bun:sqlite and better-sqlite3's prepare method's type means that TypeScript considers the two to be incompatible (even though I'm calling them in a way that works for both). I completely bail (also because my Emacs is crashing when completing with LSP for some reason), and just copy the implementation for Bun back into the main branch, duplicating the file. This lets the scraping continue while I figure out how to actually solve it.

* 2024-07-22

I make the second try to make the one file support both runtimes and both SQLite libraries. There is https://github.com/farjs/better-sqlite3-wrapper/, but I instead just use a type assertion to pretend the better-sqlite3 database constructor is an instance of Bun's Database object. This makes TypeScript not protect me against using the wrong methods, but I can still catch that by just running the code. It also allows TypeScript's completion to continue working.

Use concurrency for slugArrayFile instead of relying on multiple processes. This avoids the duplicating the overhead of each instance of the scraper process. The only thing that meaningfully benefits from running in parallel is the network requests, and we still get that parallelism with promises.

Overhaul the interface for scraping mentioned URLs, adding the extract-then-scrape action to the scraper itself and not duct taped through Make. Also made it show how many mentioned URLs that are about to be scraped. Fun fact: for some reason a significant portion of these more-than-one-level-deep links all resolve to the same hosts. For 4 digit slugs a ton of them end up resolving to a sketchy-sounding Facebook app; for 5 digit slugs a ton of them resolve to a bare IP starting with 50.

* 2024-07-23

When a job finishes, write the job description down. I make each fetch output the result to the terminal so that I can gauge how fast or slow things are going, but that means I wouldn't be able to see if a job has finished. Writing them down in a file makes this much simpler.

* 2024-07-24

Already append the query string to tell goo.gl to not return the “interstitial page”. Might as well do it now.

Write a help text so I don't have to open the source code and navigate to the right place to see what commands I've made available.

Make the slug ordering 0-9A-Za-z (uppercase then lowercase) so that it agrees with the string sorting elsewhere: in Emacs (for ordering tasks), in SQLite (for checking the progress of each “block” in a rudimentary way), etc.

Give up doing anything on C, as the amount of data that would be scraped and stored would be annoying to transfer out of Termux's home directory.

* 2024-07-25

Save a few object references: instead of reusing obj.key multiple times, store it into a variable then reference the variable. Afaik this is one of those optimizations that's guaranteed to be beneficial, except the benefit is minuscule and usually not worth the hassle. This wouldn't be much relative to the total amount of compute, but since the total compute amount is /a hell lot/ in absolute terms this probably does add up.

Remove the external slugs mechanism to try to reduce memory use. The mechanism does not scale well, since at a certain point there are so many slugs, trying to fit them all into memory is just no longer worth it. This apparently saves a bit of memory.

Then implement a new way to specify jobs. Instead of launching multiple processes, each responsible for a block defined by the command, I implemented a way to specify multiple blocks in a JSON file. This way one process would be able to read multiple jobs. Just as for slugArrayFile, only network requests need to actually be parallel, and they always are, so invoking the async scrape function in JS for each “job” is much more efficient. This massively reduces memory use: about 100MB per process before and after, but before this change we needed one process per job, while after this change every job runs in this process.

The bottleneck is now network speed and bandwidth, as well as disk space for the database.

* 2024-07-26

Made a command for restarting for A. It didn't really work.

* 2024-07-27

Added a script for checking the current rate of storing slugs. We basically check the current slug count and compare with the slug count like 10 seconds ago, and average it out to get a per second figure. Counting the exact total number is slow, so I resort to using the highest rowid. The rowid difference should still be just as accurate as the difference of the exact number of rows.

Tried to implement conversion between slugs and integers. This would make it easier to divide blocks of slugs when balancing them out.

This is somewhat more complicated than just converting between base-10 and base-62. “0001” is the same number as “1”, but for slugs they are different.

As I have a ground truth “next” function, I could just count up from 0 for both cases, but this predictably blows up due to time complexity.

Nevertheless, this is enough for me to finally divide blocks somewhat evenly. At this point the number of blocks is still the number of concurrent fetches that can happen, and if a block finishes before another one we just end up doing less fetches at the same time, so blocks really need to be evenly divided.

I wrote a =dividePortions= function for me to do that, albeit still kind of manually (as I divide them up and write them into the “jobs” file).

Write the jobs in TypeScript files instead of in JSON so that we can have comments. Adding logic in there would also be fine if I find a need for them.

* 2024-07-28

Tried to write another version of =numberToSlug= that does some math instead of brute forcing it. I called it “seemingly not broken” in the commit message, but it turns out to break around the last slug character.

I was hoping =slugToNumber= and =numberToSlug= can be used to reliably split up jobs, so I can both scrape a block of slugs somewhat evenly and also do so concurrently. But getting =numberToSlug= to work proved to be too difficult.

I took another approach instead. Each concurrent fetching “thread” can simply receive the next slug to scrape from a central iterator, so that concurrent fetches are not tied to the number of jobs. This means defining blocks and balancing them out evenly is no longer necessary.

Blocks are automatically evenly divided: each block definition is turned into an iterator that takes returns each slug within the block in order, and these iterators are then combined with =roundRobin= (honestly a questionable name, whatever) from =iter-tools-es=, which just returns the first value from the first iterator, first value from the second, …, second value from the first, second value from the second, etc. until all iterators are exhausted.

Then we just keep ticking through the same iterator in multiple promises, and the result is a predictable number of fetches happening at the same time.

This then means we can allow many concurrent fetches — much more than what I was previously doing. I was doing maybe 8 or maybe 16 blocks at once. At this point I realize it's possible to do 128 concurrent fetches at once, even on my 4G internet. This means going from something like 30 slugs a second to over 100. We're now only limited by my network speed and the rate goo.gl is willing to serve me at, and it's very easy to scale up to whatever limit there may be.

* 2024-07-29 ~ 2024-07-31

Fix some edge case errors, like how slug ranges didn't include the end.

Add some more progress reporting options.

* 2024-08-01

Report user-agent to, like, be a good internet citizen.

Also allow A to handle more (300) concurrent fetches. We're only really limited on network speed, and A has really fast internet access. 300 concurrent fetches turns out to be like 1100 slugs per second, and is able to go through multiple 5-character blocks (like F0000~J0000, which is 59105344 slugs) in like 15 hours. The limiting factor isn't even speed now (at least unless I want to scale to 6-character blocks), it is disk space, as A is also constrained on that.

* 2024-08-02

Try using different journal modes when merging multiple databases. I'm still not sure how much time this actually saves.

Also treat 403 codes as having already scraped.

Make it possible to verify progress in a separate file with all the data combined.

Prepare to try to move remote data onto my phone after it is done on A, as I leave home for COSCUP.

* 2024-08-04 ~ 2024-08-13

Scraping on A and moving them onto my phone works so well that I've given up even trying to scrape on B.

All the bare 5-digit blocks are done at this point, though the resulting 50 GiB of data that's sitting on my phone's SD card still needs to be moved and merged back into the main database. (Keeping 50 GiB of data on an SD card is kind of questionable).
